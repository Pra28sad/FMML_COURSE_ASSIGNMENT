{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_7x3NrlFGo78"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pra28sad/FMML_COURSE_ASSIGNMENT/blob/main/FMML_Aug'22_M9_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundation of Modern Machine Learning\n",
        "## Module 9: Neural Networks\n",
        "## Lab 2: Using MLP for multiclass classification\n",
        "#### Module Coordinator: Shantanu Agrawal\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Og_zWt_sEVKz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP_76wepF6lj"
      },
      "source": [
        "Till now, we got to know what is MLP, how it can be used for classification. We have done the single class classification in the previous lab session.\n",
        "\n",
        "In this notebook we will try to use an MLP for multiclass classification on the iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tWls1xEs4IC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzTYvf9atkU6"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0JZiPaNtjWl"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']\n",
        "\n",
        "# Scale data to have mean 0 and variance 1\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6j7J1YWtrLm"
      },
      "source": [
        "# Visualising dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD1UuMr_txc1"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "for target, target_name in enumerate(names):\n",
        "    X_plot = X[y == target]\n",
        "    ax1.plot(X_plot[:, 0], X_plot[:, 1], \n",
        "             linestyle='none', \n",
        "             marker='o', \n",
        "             label=target_name)\n",
        "ax1.set_xlabel(feature_names[0])\n",
        "ax1.set_ylabel(feature_names[1])\n",
        "ax1.axis('equal')\n",
        "ax1.legend();\n",
        "\n",
        "for target, target_name in enumerate(names):\n",
        "    X_plot = X[y == target]\n",
        "    ax2.plot(X_plot[:, 2], X_plot[:, 3], \n",
        "             linestyle='none', \n",
        "             marker='o', \n",
        "             label=target_name)\n",
        "ax2.set_xlabel(feature_names[2])\n",
        "ax2.set_ylabel(feature_names[3])\n",
        "ax2.axis('equal')\n",
        "ax2.legend();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7x3NrlFGo78"
      },
      "source": [
        "# Observing the dataset\n",
        "\n",
        "Thus, we can observe the dataset and see that there are *3 classes, setosa, versicolor, and virginica*.\n",
        "\n",
        "There are *4 features, sepal width, sepal length, petal width, petal length*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZROKwQi0t7T5"
      },
      "source": [
        "# MLP for multiclass classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfnKnpVitz3s"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL7l9Mk_uFxG"
      },
      "source": [
        "# Defining the model architecture\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer2 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HSl1N4uI2f"
      },
      "source": [
        "# Instantiating the model, using Adam optimiser, and Cross Entropy Loss, which is quite commonlu used for classification tasks.\n",
        "model     = Model(X_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn   = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3mTXyDduQaY"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcLoeG9XuML9"
      },
      "source": [
        "# Train for 100 epochs\n",
        "EPOCHS  = 100\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "X_test= torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "\n",
        "loss_list     = np.zeros((EPOCHS,))\n",
        "accuracy_list = np.zeros((EPOCHS,))\n",
        "\n",
        "for epoch in tqdm.trange(EPOCHS):\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss_list[epoch] = loss.item()\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "        accuracy_list[epoch] = correct.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQdMVtTouqnl"
      },
      "source": [
        "# Plot training progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTNRq4mDutrQ"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax1.plot(accuracy_list)\n",
        "ax1.set_ylabel(\"validation accuracy\")\n",
        "ax2.plot(loss_list)\n",
        "ax2.set_ylabel(\"validation loss\")\n",
        "ax2.set_xlabel(\"epochs\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZO8zVx2DOz"
      },
      "source": [
        "# Experiment with the neural network architecture\n",
        "\n",
        "\n",
        "1.   Try changing the number of hidden layers.\n",
        "2.   Try changing the number of neurons in the hidden layer.\n",
        "3.   Try using a different activation function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Can you observe any changes?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solutions 1.Try changing the number of hidden layers.\n",
        "\n",
        "Adding 4 more hidden layers"
      ],
      "metadata": {
        "id": "LKLAvmAw9CLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Defining the model architecture\n",
        "class Model_exp1(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model_exp1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer2_h1 = nn.Linear(50, 50)\n",
        "        self.layer2_h2 = nn.Linear(50, 50)\n",
        "        self.layer2_h3 = nn.Linear(50, 50)\n",
        "        self.layer2_h4 = nn.Linear(50, 50)\n",
        "        self.layer2_h5 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2_h1(x))\n",
        "        x = F.relu(self.layer2_h2(x))\n",
        "        x = F.relu(self.layer2_h3(x))\n",
        "        x = F.relu(self.layer2_h4(x))\n",
        "        x = F.relu(self.layer2_h5(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiating the model, using Adam optimiser, and Cross Entropy Loss, which is quite commonlu used for classification tasks.\n",
        "model     = Model_exp1(X_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn   = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train for 100 epochs\n",
        "EPOCHS  = 100\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "X_test= torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "\n",
        "loss_list = np.zeros((EPOCHS,))\n",
        "accuracy_list_train = np.zeros((EPOCHS,))\n",
        "accuracy_list_test = np.zeros((EPOCHS,))\n",
        "\n",
        "for epoch in tqdm.trange(EPOCHS):\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss_list[epoch] = loss.item()\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Test Accuracy\n",
        "        y_pred = model(X_test)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "        accuracy_list_test[epoch] = correct.mean()\n",
        "\n",
        "        # Train Accuracy\n",
        "        y_pred = model(X_train)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_train).type(torch.FloatTensor)\n",
        "        accuracy_list_train[epoch] = correct.mean()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax1.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "ax1.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "ax1.legend()\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax2.plot(loss_list)\n",
        "ax2.set_ylabel(\"validation loss\")\n",
        "ax2.set_xlabel(\"epochs\")\n",
        "     "
      ],
      "metadata": {
        "id": "iRHs0_dew63e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "plt.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylim(0.8, 1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cssvfWN4xcfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding 4 more layers, the model parameters converged and last plot shows that model is bit over fitting\n",
        "\n",
        "Removing hidden layers"
      ],
      "metadata": {
        "id": "jsu_XwxH9wWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# Defining the model architecture\n",
        "class Model_exp1(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model_exp1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiating the model, using Adam optimiser, and Cross Entropy Loss, which is quite commonlu used for classification tasks.\n",
        "model     = Model_exp1(X_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn   = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train for 100 epochs\n",
        "EPOCHS  = 100\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "X_test= torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "\n",
        "loss_list = np.zeros((EPOCHS,))\n",
        "accuracy_list_train = np.zeros((EPOCHS,))\n",
        "accuracy_list_test = np.zeros((EPOCHS,))\n",
        "\n",
        "for epoch in tqdm.trange(EPOCHS):\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss_list[epoch] = loss.item()\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Test Accuracy\n",
        "        y_pred = model(X_test)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "        accuracy_list_test[epoch] = correct.mean()\n",
        "\n",
        "        # Train Accuracy\n",
        "        y_pred = model(X_train)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_train).type(torch.FloatTensor)\n",
        "        accuracy_list_train[epoch] = correct.mean()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "ax1.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "ax1.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "ax1.legend()\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_ylim(0, 1.1)\n",
        "ax2.plot(loss_list)\n",
        "ax2.set_ylabel(\"validation loss\")\n",
        "ax2.set_xlabel(\"epochs\")"
      ],
      "metadata": {
        "id": "FcV6urSyxuVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "plt.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylim(0.8, 1.1)\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "IDH50HCdyIag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model didn't performe well as compared to model with one hidden layer. Hidden layers in between 2 and 4 seems to be an good option given that number of neurons in one hidden layer remains same.\n",
        "\n",
        "2.Try changing the number of neurons in the hidden layer."
      ],
      "metadata": {
        "id": "lUGPdQBX-AZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture\n",
        "class Model_exp2(nn.Module):\n",
        "    def __init__(self, input_dim, neurons):\n",
        "        super(Model_exp2, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, neurons)\n",
        "        self.layer2 = nn.Linear(neurons, neurons) \n",
        "        self.layer3 = nn.Linear(neurons, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiating the model, using Adam optimiser, and Cross Entropy Loss, which is quite commonlu used for classification tasks.\n",
        "neurons = [50, 100, 1000, 10000]\n",
        "for neuron_count in neurons:\n",
        "  # Preparing Dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)\n",
        "  print(f\"================== Number of neuron in HL = {neuron_count} ==================\")\n",
        "  model     = Model_exp2(X_train.shape[1], neuron_count)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  loss_fn   = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train for 100 epochs\n",
        "  EPOCHS  = 100\n",
        "  X_train = torch.from_numpy(X_train).float()\n",
        "  X_test= torch.from_numpy(X_test).float()\n",
        "  y_test = torch.from_numpy(y_test)\n",
        "  y_train = torch.from_numpy(y_train)\n",
        "\n",
        "\n",
        "  loss_list = np.zeros((EPOCHS,))\n",
        "  accuracy_list_train = np.zeros((EPOCHS,))\n",
        "  accuracy_list_test = np.zeros((EPOCHS,))\n",
        "\n",
        "  for epoch in tqdm.trange(EPOCHS):\n",
        "      y_pred = model(X_train)\n",
        "      loss = loss_fn(y_pred, y_train)\n",
        "      loss_list[epoch] = loss.item()\n",
        "      \n",
        "      # Zero gradients\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          # Test Accuracy\n",
        "          y_pred = model(X_test)\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "          accuracy_list_test[epoch] = correct.mean()\n",
        "\n",
        "          # Train Accuracy\n",
        "          y_pred = model(X_train)\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_train).type(torch.FloatTensor)\n",
        "          accuracy_list_train[epoch] = correct.mean()\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "  ax1.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "  ax1.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "  ax1.legend()\n",
        "  ax1.set_ylabel(\"Accuracy\")\n",
        "  ax1.set_ylim(0, 1.1)\n",
        "  ax2.plot(loss_list)\n",
        "  ax2.set_ylabel(\"validation loss\")\n",
        "  ax2.set_xlabel(\"epochs\")\n",
        "  plt.show()\n",
        "\n",
        "# Defining the model architecture\n",
        "class Model_exp2(nn.Module):\n",
        "    def __init__(self, input_dim, neurons):\n",
        "        super(Model_exp2, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, neurons)\n",
        "        self.layer2 = nn.Linear(neurons, neurons) \n",
        "        self.layer3 = nn.Linear(neurons, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiating the model, using Adam optimiser, and Cross Entropy Loss, which is quite commonlu used for classification tasks.\n",
        "neurons = [50, 100, 1000, 10000]\n",
        "for neuron_count in neurons:\n",
        "  # Preparing Dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)\n",
        "  print(f\"================== Number of neuron in HL = {neuron_count} ==================\")\n",
        "  model     = Model_exp2(X_train.shape[1], neuron_count)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  loss_fn   = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train for 100 epochs\n",
        "  EPOCHS  = 100\n",
        "  X_train = torch.from_numpy(X_train).float()\n",
        "  X_test= torch.from_numpy(X_test).float()\n",
        "  y_test = torch.from_numpy(y_test)\n",
        "  y_train = torch.from_numpy(y_train)\n",
        "\n",
        "\n",
        "  loss_list = np.zeros((EPOCHS,))\n",
        "  accuracy_list_train = np.zeros((EPOCHS,))\n",
        "  accuracy_list_test = np.zeros((EPOCHS,))\n",
        "\n",
        "  for epoch in tqdm.trange(EPOCHS):\n",
        "      y_pred = model(X_train)\n",
        "      loss = loss_fn(y_pred, y_train)\n",
        "      loss_list[epoch] = loss.item()\n",
        "      \n",
        "      # Zero gradients\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          # Test Accuracy\n",
        "          y_pred = model(X_test)\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "          accuracy_list_test[epoch] = correct.mean()\n",
        "\n",
        "          # Train Accuracy\n",
        "          y_pred = model(X_train)\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_train).type(torch.FloatTensor)\n",
        "          accuracy_list_train[epoch] = correct.mean()\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "  ax1.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "  ax1.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "  ax1.legend()\n",
        "  ax1.set_ylabel(\"Accuracy\")\n",
        "  ax1.set_ylim(0, 1.1)\n",
        "  ax2.plot(loss_list)\n",
        "  ax2.set_ylabel(\"validation loss\")\n",
        "  ax2.set_xlabel(\"epochs\")\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NZ5Bvg43ybB9",
        "outputId": "611dddaf-800a-4dbb-8cb4-2d46ecd3ce9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 19/100 [03:47<15:03, 11.15s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we increased more more number of neuron the model started to behave similar we increased more HL, faster convergence, higher accuracy, longer training time and tendency to over fit\n",
        "\n",
        "3.Try using a different activation function."
      ],
      "metadata": {
        "id": "rX210GN5-NCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model architecture\n",
        "class Model_exp3(nn.Module):\n",
        "    def __init__(self, input_dim, act_function):\n",
        "        super(Model_exp3, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer2 = nn.Linear(50, 50) \n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        self.activation = act_function\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.layer1(x))\n",
        "        x = self.activation(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiating the model, using Adam optimiser, and Cross Entropy Loss, which is quite commonlu used for classification tasks.\n",
        "act_functions = [F.relu, F.tanh, F.sigmoid]\n",
        "for act in act_functions:\n",
        "  # Preparing Dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=2)\n",
        "  print(f\"================== Activation Functions is {act} ==================\")\n",
        "  model     = Model_exp3(X_train.shape[1], act)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "  loss_fn   = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train for 100 epochs\n",
        "  EPOCHS  = 100\n",
        "  X_train = torch.from_numpy(X_train).float()\n",
        "  X_test= torch.from_numpy(X_test).float()\n",
        "  y_test = torch.from_numpy(y_test)\n",
        "  y_train = torch.from_numpy(y_train)\n",
        "\n",
        "\n",
        "  loss_list = np.zeros((EPOCHS,))\n",
        "  accuracy_list_train = np.zeros((EPOCHS,))\n",
        "  accuracy_list_test = np.zeros((EPOCHS,))\n",
        "\n",
        "  for epoch in tqdm.trange(EPOCHS):\n",
        "      y_pred = model(X_train)\n",
        "      loss = loss_fn(y_pred, y_train)\n",
        "      loss_list[epoch] = loss.item()\n",
        "      \n",
        "      # Zero gradients\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          # Test Accuracy\n",
        "          y_pred = model(X_test)\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "          accuracy_list_test[epoch] = correct.mean()\n",
        "\n",
        "          # Train Accuracy\n",
        "          y_pred = model(X_train)\n",
        "          correct = (torch.argmax(y_pred, dim=1) == y_train).type(torch.FloatTensor)\n",
        "          accuracy_list_train[epoch] = correct.mean()\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
        "\n",
        "  ax1.plot(accuracy_list_train, label=\"Train accuracy\")\n",
        "  ax1.plot(accuracy_list_test, label=\"Test accuracy\")\n",
        "  ax1.legend()\n",
        "  ax1.set_ylabel(\"Accuracy\")\n",
        "  ax1.set_ylim(0, 1.1)\n",
        "  ax2.plot(loss_list)\n",
        "  ax2.set_ylabel(\"validation loss\")\n",
        "  ax2.set_xlabel(\"epochs\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "lyJ3enKh1Bhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above plot shows the variation loss/accuracy with change in activation functions, namely relu, tanh, sigmoid. In this particular case, relu seems to be a better option since it has the least fluctuations in accuracy and faster convergence"
      ],
      "metadata": {
        "id": "q17kH0Xj-adI"
      }
    }
  ]
}